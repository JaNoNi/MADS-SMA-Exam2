{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c46190-c789-460f-b4f6-2b9baa11fc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from collections import Counter\n",
    "from itertools import chain, combinations\n",
    "from json import JSONDecodeError\n",
    "\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import spacy\n",
    "from community import community_louvain\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "from pyvis.network import Network\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b71a604-04e4-4677-8269-8ea8612f6859",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(sns.color_palette(\"ch:s=2.5,rot=.15\"))\n",
    "cmap = sns.color_palette((\"ch:s=2.5,rot=.15\"), as_cmap=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c235300-f247-449c-a744-53d2c36ae7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"data/slapgate_twitter_mymodel2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63de7358-1562-4a82-b040-dd725be2a330",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[:2, :20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f984f7ee-805a-4b0e-88e1-19a76342ae16",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[:5, 20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c7645d-8b65-46cd-9d8a-cf0ec073b186",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.figure(figsize=(15, 6), dpi=90)\n",
    "ax = sns.countplot(data=df, x=\"mymodel2_label\")\n",
    "ax.set_xlabel(\"Emotions\")\n",
    "ax.set_ylabel(\"Count\")\n",
    "for p in ax.patches:\n",
    "    ax.annotate(\"{:>5,d}\".format(p.get_height()), (p.get_x() + 0.3, p.get_height() + 100))\n",
    "ax.xaxis.set_tick_params(which=\"both\", labelleft=True)\n",
    "ax.set_title(\"Predictions using 'my distilbert-model2'\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77eacee6-0871-41ca-b045-09794036d3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "others = df.groupby(\"source\").count().sort_values(\"_id\", ascending=False).index[5:]\n",
    "df_source = df.replace(others, \"Other\")\n",
    "df_src_grp = df_source.groupby([\"source\", \"mymodel2_label\"]).count().reset_index().iloc[:, :3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78e2093-6a2c-4eaa-a664-e0601b2a8acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_src_grp_sum = df_src_grp.groupby(\"source\").agg([\"count\", \"sum\"])\n",
    "df_src_grp_sum.columns = [\"_\".join(col) for col in df_src_grp_sum.columns.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1f9b8a-896e-4f02-badc-20568f9063aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_src_grp_t = pd.merge(\n",
    "    df_src_grp, df_src_grp_sum.reset_index()[[\"source\", \"_id_sum\"]], on=\"source\"\n",
    ")\n",
    "df_src_grp_t[\"_id_perc\"] = df_src_grp_t[\"_id\"] / df_src_grp_t[\"_id_sum\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa67b06-6690-48ed-b5bf-47382d10c0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.figure(figsize=(15, 6), dpi=90)\n",
    "ax = sns.barplot(data=df_src_grp_t, x=\"mymodel2_label\", y=\"_id_perc\", hue=\"source\")\n",
    "ax.set_xlabel(\"Emotions\")\n",
    "ax.set_ylabel(\"%-Emotion\")\n",
    "ax.yaxis.set_major_formatter(PercentFormatter(xmax=1))\n",
    "ax.set_title(\"Predicted Sentiment per source\")\n",
    "ax.text(1, 0.45, \"Instragram Tweets\")\n",
    "plt.plot([0.7, 0.99], [0.5, 0.455], \"k-\", lw=1)\n",
    "plt.plot([1.53, 1.65], [0.455, 0.06], \"k-\", lw=1)\n",
    "ax.text(3, 0.21, \"Smartphones\")\n",
    "plt.plot([3.15, 3.05], [0.2, 0.06], \"k-\", lw=1)\n",
    "plt.plot([3.25, 3.35], [0.2, 0.06], \"k-\", lw=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db33b3bd-8eb2-43e3-adc2-94ba65acef25",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "For our preprocessing steps we will correct two aspects. The first is to identify and remove potential spam accounts and the second is to make the tweets' text more machine readable. For the possible spam accounts we remove accounts that have posted an excessive amount of tweets. Below we can see that we have accounts with more 100 tweets in our short time frame. Although some power users might sent this many tweets, I will remove those frequent posters in an attempt to get rid of possible spam tweets posted by (probably) bots. Below are some example spam-tweets. Those 149 accounts are responsible for ~3.500 tweets. More than half of these tweets have been posted via the Web App, which speaks for a possible bot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9baac9e6-b9fe-4b3b-acec-1862dc975ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped = df.groupby(\"author_id\").count().sort_values(by=\"_id\", ascending=False)\n",
    "spammer_ids = df_grouped[df_grouped[\"_id\"] >= 10].reset_index()[\"author_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d665fc-6783-4fe7-9c99-bf20ceb615f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nobots = df.drop(df[df[\"author_id\"].isin(spammer_ids)].index, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19906c3-d851-4d8d-9a4e-f9403e4eb28f",
   "metadata": {},
   "source": [
    "Next we will edit the tweets themselves. Many users have used the special character & which is encoded as & and will be converted to the word and. Additionally we remove all '@' and '#' symbols to make the text more readable. The example below shows how many users use hashtags and @ as part of their speech in the tweets. Therefore, completely removing the hashtags would remove some meaning of the origin text. We also remove all hyperlinks from the tweets because those produce many false positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89075161-0614-44a0-a67c-c79c1dd632ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc_tweets(txt: str):\n",
    "    new_txt = txt.replace(\"&amp;\", \"and\")\n",
    "    new_txt = new_txt.replace(\"@\", \"\")\n",
    "    new_txt = new_txt.replace(\"#\", \"\")\n",
    "    new_txt = re.sub(\n",
    "        r\"(http|https|ftp|ftps)\\:\\/\\/[a-zA-Z0-9\\-\\.]+\\.[a-zA-Z]{2,3}(\\/\\S*)?\", \"\", new_txt\n",
    "    )\n",
    "    return new_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87b0b85-2d4d-4694-afe7-46f7d84ff85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text\"].loc[7763:7764].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686848ba-a0a2-435e-9a87-605dd135f294",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text\"].loc[7763:7764].apply(preproc_tweets).iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b209d9d-43bc-4745-89b4-b5d28352246b",
   "metadata": {},
   "source": [
    "We apply our preprocessing steps on the tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f10651-c3ab-40fd-a131-8e6508b81f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text\"] = df[\"text\"].apply(preproc_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea81994-66d4-47b9-8867-a40aff1beaf3",
   "metadata": {},
   "source": [
    "## Network Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85346f8d-fee2-4fc1-9213-46db720436fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835a133d-7738-4ec7-8252-7db7731b2e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "verbs = list()\n",
    "adj = list()\n",
    "propn = list()\n",
    "for idx in tqdm(range(len(df))):\n",
    "    _id = df.iloc[idx, :][\"_id\"]\n",
    "    doc = nlp(df.iloc[idx, :][\"text\"])\n",
    "    for token in doc:\n",
    "        if token.pos_ == \"VERB\":\n",
    "            verbs.append([_id, token.text])\n",
    "        elif token.pos_ == \"ADJ\":\n",
    "            adj.append([_id, token.text])\n",
    "        elif token.pos_ == \"PROPN\":\n",
    "            propn.append([_id, token.text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b841726a-16db-4dc8-96be-8d8c0e0dc7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_verbs = pd.DataFrame(verbs, columns=[\"ids\", \"verbs\"])\n",
    "df_adj = pd.DataFrame(adj, columns=[\"ids\", \"adjectives\"])\n",
    "df_propn = pd.DataFrame(propn, columns=[\"ids\", \"porper_noun\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475850b4-fc00-45d0-8bb2-f3f3ffd583e8",
   "metadata": {},
   "source": [
    "### Clean the POSs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228330ff-86d8-49ca-9b3f-e4718ce6a805",
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = (\n",
    "    df_verbs[\"verbs\"].str.startswith(\"’\")\n",
    "    | df_verbs[\"verbs\"].str[0].str.isupper()\n",
    "    | df_verbs[\"verbs\"].str.contains(\"smith\")\n",
    "    | df_verbs[\"verbs\"].str.contains(\"chris\")\n",
    "    | df_verbs[\"verbs\"].str.contains(\"oscar\")\n",
    "    | (df_verbs[\"verbs\"].str.len() <= 2)\n",
    ")\n",
    "df_verbs = df_verbs.drop(df_verbs[conditions].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315aa2bf-3431-441b-88ce-c5f69c8a2a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = (\n",
    "    df_adj[\"adjectives\"].str.startswith(\"’\")\n",
    "    | df_adj[\"adjectives\"].str[0].str.isupper()\n",
    "    | df_adj[\"adjectives\"].str.contains(\"smith\")\n",
    "    | df_adj[\"adjectives\"].str.contains(\"chris\")\n",
    "    | df_adj[\"adjectives\"].str.contains(\"oscar\")\n",
    "    | (df_adj[\"adjectives\"].str.len() <= 2)\n",
    ")\n",
    "df_adj = df_adj.drop(df_adj[conditions].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a83a9f-60b3-4202-bb87-968e6e63dd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = df_propn[\"porper_noun\"].str.len() <= 2\n",
    "df_propn = df_propn.drop(df_propn[conditions].index)\n",
    "df_propn[\"porper_noun\"] = df_propn[\"porper_noun\"].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b83231b-7f58-4da5-a37b-1f4f95b0751e",
   "metadata": {},
   "source": [
    "### Create Nodes and Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c922d8c-d37f-4d62-b0fb-7bc99416f2e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_nodes(df: pd.DataFrame, tag: str, count: int = 100) -> pd.DataFrame:\n",
    "    _df = df.copy(deep=True)\n",
    "    nodes = _df.value_counts(tag).reset_index().rename(columns={0: \"n\"})\n",
    "    nodes[\"position\"] = nodes[\"n\"].rank(ascending=False).astype(int)\n",
    "    nodes = nodes[nodes[\"position\"] <= count]\n",
    "    return nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882c38ea-7af1-464b-9a47-9e9315965d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_edges(\n",
    "    df: pd.DataFrame, nodes: pd.DataFrame, tag: str, mention_n: int = 2\n",
    ") -> pd.DataFrame:\n",
    "    _df = df.merge(nodes[tag], how=\"inner\", on=tag)\n",
    "    _df[f\"n_{tag}\"] = _df.groupby(\"ids\")[tag].transform(\"count\")\n",
    "    _df = _df[_df[f\"n_{tag}\"] >= mention_n].reset_index().drop(columns=f\"n_{tag}\")\n",
    "    _df[tag] = _df[tag].astype(str)\n",
    "    _df[tag] = _df.groupby([\"ids\"])[tag].transform(lambda x: \" \".join(x))\n",
    "    _df = _df.reset_index().drop_duplicates(subset=\"ids\")[[\"ids\", tag]].reset_index(drop=True)\n",
    "    _df[tag] = _df[tag].map(str.split)\n",
    "\n",
    "    tag_pairs = [list(combinations(i, 2)) for i in _df[tag]]\n",
    "    all_tag_pairs = list(chain(*tag_pairs))\n",
    "    tag_pair_count = Counter(all_tag_pairs)\n",
    "\n",
    "    edges = pd.DataFrame.from_dict(tag_pair_count, orient=\"index\")\n",
    "    edges[f\"{tag}1\"], edges[f\"{tag}2\"] = zip(*edges.index)\n",
    "    edges.reset_index(inplace=True, drop=True)\n",
    "    edges = edges.rename(columns={0: \"value\"}).sort_values(\"value\", ascending=False)\n",
    "    cols = [f\"{tag}1\", f\"{tag}2\", \"value\"]\n",
    "    edges = edges[cols]\n",
    "    edges = (\n",
    "        edges.merge(nodes, how=\"inner\", left_on=f\"{tag}1\", right_on=tag)\n",
    "        .rename(columns={\"n\": \"source_n\"})\n",
    "        .merge(nodes, how=\"inner\", left_on=f\"{tag}2\", right_on=tag)\n",
    "        .rename(columns={\"n\": \"target_n\"})\n",
    "    )\n",
    "\n",
    "    edges[f\"{tag}1_share\"] = edges.value / edges.source_n\n",
    "    edges[f\"{tag}2_share\"] = edges.value / edges.target_n\n",
    "\n",
    "    edges = edges.query(f\"{tag}1_share>0.05 | {tag}2_share>0.05\")\n",
    "    return edges[[f\"{tag}1\", f\"{tag}2\", \"value\", f\"{tag}1_share\", f\"{tag}2_share\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b86367-b2c6-4cd9-8d49-7aa551d22287",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_partition(G, tag: str, nodes: pd.DataFrame) -> pd.DataFrame:\n",
    "    partition = community_louvain.best_partition(G)\n",
    "    louvain_communities = (\n",
    "        pd.DataFrame.from_dict(partition, orient=\"index\")\n",
    "        .reset_index()\n",
    "        .rename(columns={\"index\": tag, 0: \"louvain\"})\n",
    "    )\n",
    "    nodes = nodes.merge(louvain_communities, how=\"inner\", on=tag)\n",
    "    return nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fcdaf6-d963-4c35-aad4-226d7155594d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag = \"verbs\"\n",
    "verb_nodes = create_nodes(df_verbs, tag, 100)\n",
    "verb_nodes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566fe2c8-8b98-4c58-b97e-869476144781",
   "metadata": {},
   "outputs": [],
   "source": [
    "verb_edges = create_edges(df_verbs, verb_nodes, tag, mention_n=2)\n",
    "verb_edges.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424b0774-78cd-4105-88d3-1fd2e74ae6a2",
   "metadata": {},
   "source": [
    "### Create Network Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b316f0-a838-461f-ac32-df67449af493",
   "metadata": {},
   "outputs": [],
   "source": [
    "verb_G = nx.Graph()\n",
    "for index, row in verb_nodes.iterrows():\n",
    "    verb_G.add_node(row[tag], nodesize=row[\"n\"])\n",
    "for index, row in verb_edges.iterrows():\n",
    "    verb_G.add_edge(row[f\"{tag}1\"], row[f\"{tag}2\"], weight=row[\"value\"])\n",
    "verb_nodes_l = create_partition(verb_G, tag, verb_nodes)\n",
    "verb_nodes_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a725274-fcd8-462f-92f5-804823aafc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_graph(G, size, nodes, k=3, iterations=60):\n",
    "    plt.figure(figsize=size)\n",
    "    pos = nx.drawing.spring_layout(G, k=k, iterations=iterations)\n",
    "    node_size = [d[\"nodesize\"] * 0.7 for _, d in G.nodes(data=True)]\n",
    "    edge_width = [np.sqrt(d[\"weight\"] * 0.05) for _, _, d in G.edges(data=True)]\n",
    "    cmap = cm.get_cmap(\"coolwarm\", nodes.louvain.nunique())\n",
    "\n",
    "    # Draw Network\n",
    "    nx.draw_networkx(\n",
    "        G,\n",
    "        pos=pos,\n",
    "        node_color=nodes.louvain,\n",
    "        cmap=cmap,\n",
    "        node_size=node_size,\n",
    "        width=edge_width,\n",
    "        edge_color=\"grey\",\n",
    "        font_size=16,\n",
    "        alpha=0.8,\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa083594-9a75-4124-b16b-f9ab39a37b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_graph(verb_G, size=(20, 12), nodes=verb_nodes_l, k=4, iterations=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb58d79-fea4-4f01-800f-64c120740def",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag = \"adjectives\"\n",
    "adj_nodes = create_nodes(df_adj, tag, 100)\n",
    "adj_edges = create_edges(df_adj, adj_nodes, tag, 2)\n",
    "adj_G = nx.Graph()\n",
    "for index, row in adj_nodes.iterrows():\n",
    "    adj_G.add_node(row[tag], nodesize=row[\"n\"])\n",
    "for index, row in adj_edges.iterrows():\n",
    "    adj_G.add_edge(row[f\"{tag}1\"], row[f\"{tag}2\"], weight=row[\"value\"])\n",
    "adj_nodes_l = create_partition(adj_G, tag, adj_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0285fe3-753c-4457-b9a5-5a8599fdc7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_graph(adj_G, size=(20, 12), nodes=adj_nodes_l, k=3, iterations=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfd6380-e3b5-4ea0-b18c-8e3a75b2b81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag = \"porper_noun\"\n",
    "propn_nodes = create_nodes(df_propn, tag, 75)\n",
    "propn_edges = create_edges(df_propn, propn_nodes, tag, 2)\n",
    "propn_G = nx.Graph()\n",
    "for index, row in propn_nodes.iterrows():\n",
    "    propn_G.add_node(row[tag], nodesize=row[\"n\"])\n",
    "for index, row in propn_edges.iterrows():\n",
    "    propn_G.add_edge(row[f\"{tag}1\"], row[f\"{tag}2\"], weight=row[\"value\"])\n",
    "propn_nodes_l = create_partition(propn_G, tag, propn_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3858e0-6ca5-4348-bb5f-f4c07b824a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_graph(propn_G, size=(20, 12), nodes=propn_nodes_l, k=3, iterations=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a2b770-599f-4052-bb1a-3adff116a9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nt = Network(height=\"600px\", width=\"1200px\", notebook=True)\n",
    "\n",
    "for index, row in propn_nodes_l.iterrows():\n",
    "    nt.add_node(\n",
    "        n_id=row[tag],\n",
    "        value=row[\"n\"],\n",
    "        group=row[\"louvain\"],\n",
    "        shape=\"dot\",\n",
    "        title=f\"Tag: {row[tag]} \\n Rank: {row['position']}\",\n",
    "    )\n",
    "for index, row in propn_edges.iterrows():\n",
    "    nt.add_edge(row[f\"{tag}1\"], row[f\"{tag}2\"], weight=row[\"value\"])\n",
    "\n",
    "# nt.show_buttons(filter_=[\"physics\"])\n",
    "nt.barnes_hut(central_gravity=0, spring_length=400, damping=0.2)\n",
    "nt.set_options(\n",
    "    \"\"\"\n",
    "var options = {\n",
    "  \"physics\": {\n",
    "        \"maxVelocity\": 28,\n",
    "        \"minVelocity\": 0.75},\n",
    "  \"nodes\": {\"font\": {\"size\": 30}}\n",
    "}\"\"\"\n",
    ")\n",
    "nt.show(\"Slapgate.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a5a124-f69c-40db-962e-c7fab89f0992",
   "metadata": {},
   "source": [
    "## Twitters POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84c8840-5e8d-40e5-b6ab-788a7a5ebef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_annotations(row: list, threshold: float = 0.9):\n",
    "    results = list()\n",
    "    if not isinstance(row, list):\n",
    "        return results\n",
    "    for entry in row:\n",
    "        if entry[\"probability\"] > threshold:\n",
    "            results.append([entry[\"type\"], entry[\"normalized_text\"]])\n",
    "    return results\n",
    "\n",
    "\n",
    "def flatten_mentions(row: list):\n",
    "    results = list()\n",
    "    if not isinstance(row, list):\n",
    "        return results\n",
    "    return [entry[\"username\"] for entry in row]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f79a0f-2047-4fa2-8982-266fe0efd5ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"data/slapgate_twitter_mymodel2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6e935b-d436-43b4-853d-9489200b0577",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "bad_rows = list()\n",
    "for idx, entity in enumerate(df[\"entities\"]):\n",
    "    entity = entity.replace(\"'\", '\"')\n",
    "    try:\n",
    "        entity = json.loads(entity)\n",
    "    except (AttributeError, JSONDecodeError):\n",
    "        bad_rows.append(idx)\n",
    "        counter += 1\n",
    "print(counter)\n",
    "df = df.drop(df.index[bad_rows])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee42b8dd-67c0-48f0-8ee5-cb17b53c6237",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_entities = pd.json_normalize(df.iloc[:, :][\"entities\"].str.replace(\"'\", '\"').apply(json.loads))\n",
    "df_entities[\"annotations\"] = df_entities[\"annotations\"].apply(flatten_annotations)\n",
    "df_entities[\"mentions\"] = df_entities[\"mentions\"].apply(flatten_mentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951e5de4-dde0-4ec0-8de2-ec7e0822e087",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(\n",
    "    df,\n",
    "    df_entities.iloc[:, [0, 3]],\n",
    "    left_index=True,\n",
    "    right_index=True,\n",
    ").drop(\"entities\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55a4ad1-9db0-48c0-be23-4465956c4321",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = list()\n",
    "mentions = list()\n",
    "for idx in tqdm(range(len(df))):\n",
    "    _id = df.iloc[idx, :][\"_id\"]\n",
    "    anns = df.iloc[idx, :][\"annotations\"]\n",
    "    annotations += [[_id, an[0], an[1]] for an in anns]\n",
    "    mens = df.iloc[idx, :][\"mentions\"]\n",
    "    mentions += [[_id, men] for men in mens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaec0e34-880e-432c-b44c-368315efa524",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_annotations = pd.DataFrame(annotations, columns=[\"ids\", \"type\", \"pnoun\"])\n",
    "df_mentions = pd.DataFrame(mentions, columns=[\"ids\", \"mentions\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28835195-67e9-422a-9ca0-9888f8a04ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_annotations[\"type\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7330d6-8120-40d3-a366-1ae435a4fb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag = \"mentions\"\n",
    "mentions_nodes = create_nodes(df_mentions, tag, 75)\n",
    "mentions_edges = create_edges(df_mentions, mentions_nodes, tag, 2)\n",
    "mentions_G = nx.Graph()\n",
    "for index, row in mentions_nodes.iterrows():\n",
    "    mentions_G.add_node(row[tag], nodesize=row[\"n\"])\n",
    "for index, row in mentions_edges.iterrows():\n",
    "    mentions_G.add_edge(row[f\"{tag}1\"], row[f\"{tag}2\"], weight=row[\"value\"])\n",
    "mentions_nodes_l = create_partition(mentions_G, tag, mentions_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f40b20-4f04-491b-bfc2-c39a1397080b",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_graph(mentions_G, size=(20, 12), nodes=mentions_nodes_l, k=3, iterations=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536da27e-e75a-47b6-bec5-6c9070a2697f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag = \"pnoun\"\n",
    "annotations_nodes = create_nodes(df_annotations, tag, 75)\n",
    "annotations_edges = create_edges(df_annotations, annotations_nodes, tag, 2)\n",
    "annotations_G = nx.Graph()\n",
    "for index, row in annotations_nodes.iterrows():\n",
    "    annotations_G.add_node(row[tag], nodesize=row[\"n\"])\n",
    "for index, row in annotations_edges.iterrows():\n",
    "    annotations_G.add_edge(row[f\"{tag}1\"], row[f\"{tag}2\"], weight=row[\"value\"])\n",
    "annotations_nodes_l = create_partition(annotations_G, tag, annotations_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72de868f-bc58-4bff-b19c-813a6713e451",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_graph(annotations_G, size=(20, 12), nodes=annotations_nodes_l, k=3, iterations=60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sma",
   "language": "python",
   "name": "sma"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
